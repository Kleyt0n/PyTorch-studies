{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_softmax_crossentropy.ipynb","provenance":[],"authorship_tag":"ABX9TyMKAzY1iuemwizTYfW44uI0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WBbCdq2fuRW0"},"source":["# Softmax and Crossentropy\n","\n","## Softmax\n","\n","$$S(y_i)=\\frac{e^y_{i}}{\\sum{e^y_{i}}}$$\n","\n"," "]},{"cell_type":"code","metadata":{"id":"bX4dnnpWuPBy","executionInfo":{"status":"ok","timestamp":1627486447273,"user_tz":180,"elapsed":258,"user":{"displayName":"Kleyton Costa","photoUrl":"","userId":"09554574939383241099"}}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CcSJAOP9wDJz","executionInfo":{"status":"ok","timestamp":1627486456391,"user_tz":180,"elapsed":23,"user":{"displayName":"Kleyton Costa","photoUrl":"","userId":"09554574939383241099"}},"outputId":"b96e91d8-1210-4cf8-a6f8-1b883b482860"},"source":["# sofmax function\n","\n","def softmax(x):\n","  return np.exp(x) / np.sum(np.exp(x), axis = 0)\n","\n","x = np.array([2.0,1.0,0.1,0.4])\n","outputs = softmax(x)\n","print('softmax numpy:', outputs)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["softmax numpy: [0.58161698 0.21396493 0.08699165 0.11742644]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qqk1g4XwSMi","executionInfo":{"status":"ok","timestamp":1627486605001,"user_tz":180,"elapsed":272,"user":{"displayName":"Kleyton Costa","photoUrl":"","userId":"09554574939383241099"}},"outputId":"c9e39453-8c14-47e7-af0e-c0c067ca0241"},"source":["x = torch.tensor([2.0,1.0,0.1,0.4])\n","outputs = torch.softmax(x, dim = 0)\n","print('softmax torch:', outputs)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["softmax torch: tensor([0.5816, 0.2140, 0.0870, 0.1174])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7hF9Tzvlxj3e"},"source":["## Cross-Entropy\n","\n","$$D(\\hat{Y}, Y)=-\\frac{1}{N}\\sum Y_{i}\\times \\log(\\hat{Y_{i}})$$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWcbgsk2ygPu","executionInfo":{"status":"ok","timestamp":1627493737276,"user_tz":180,"elapsed":264,"user":{"displayName":"Kleyton Costa","photoUrl":"","userId":"09554574939383241099"}},"outputId":"43b0b547-a0e2-4901-ad32-6bbbac8a9f2a"},"source":["def cross_entropy(actual, pred):\n","  loss = -np.sum(actual*np.log(pred))\n","  return loss / float(pred.shape[0])\n","\n","Y = np.array([1,0,0])\n","\n","# y_pred prob\n","Y_pred_good = np.array([0.7, 0.2,0.1])\n","Y_pred_bad = np.array([0.1,0.3,0.6])\n","l1 = cross_entropy(Y, Y_pred_good)\n","l2 = cross_entropy(Y, Y_pred_bad)\n","\n","print(f'Loss 1 numpy: {l1:.4f}')\n","print(f'Loss 2 numpy: {l2:.4f}')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Loss 1 numpy: 0.1189\n","Loss 2 numpy: 0.7675\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hV5bJQ6qL7Er","executionInfo":{"status":"ok","timestamp":1627494256822,"user_tz":180,"elapsed":234,"user":{"displayName":"Kleyton Costa","photoUrl":"","userId":"09554574939383241099"}},"outputId":"5a9ba76e-37b3-4168-e3ea-5e64d997b24f"},"source":["# pytorch \n","\n","loss = nn.CrossEntropyLoss()\n","\n","Y = torch.tensor([0])\n","\n","# nsamples x nclasses = 1x3\n","Y_pred_good = torch.tensor([[2.0,1.0,0.3]])\n","Y_pred_bad = torch.tensor([[0.4,2.3,0.23]])\n","\n","l1 = loss(Y_pred_good,Y)\n","l2 = loss(Y_pred_bad, Y)\n","\n","print(l1.item())\n","print(l2.item())\n","\n","_, predictions1 = torch.max(Y_pred_good,1)\n","_, predict ions2 = torch.max(Y_pred_bad,1)\n","\n","print(predictions1)\n","print(predictions2)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["0.438618004322052\n","2.1435375213623047\n","tensor([0])\n","tensor([1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWbCW5iVPZNA","executionInfo":{"status":"ok","timestamp":1627495414203,"user_tz":180,"elapsed":244,"user":{"displayName":"Kleyton Costa","photoUrl":"","userId":"09554574939383241099"}},"outputId":"1c5f0237-63df-4ca9-d934-682c1a63de87"},"source":["# multiclass\n","\n","x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n","y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n","\n","n_samples, n_features = x.shape\n","print(n_samples, n_features)\n","\n","x_test = torch.tensor([5], dtype=torch.float32)\n","input_size = n_features\n","output_size = n_features\n","\n","class NeuralNet(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_classes):\n","    super(NeuralNet, self).__init__()\n","    self.linear1 = nn.Linear(input_size, hidden_size)\n","    self.relu = nn.ReLU()\n","    self.linear2 = nn.Linear(hidden_size, num_classes)\n","\n","  def forward(self,x):\n","    out = self.linear1(x)\n","    out = self.relu(out)\n","    out = self.linear2(out)\n","\n","    return out #no softmax at the end\n","\n","model = NeuralNet(input_size=input_size, hidden_size=5, num_classes = 1)\n","#criterion = nn.CrossEntropyLoss() #applies softmax\n","\n","# training\n","learning_rate = 0.01\n","n_iters = 100\n","\n","criterion =  nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) # stochastic gradient descent\n","\n","for epoch in range(n_iters):\n","  # prediction = forward pass\n","  y_pred = model(x)\n","\n","  # loss\n","  l = criterion(y, y_pred)\n","\n","  # gradients = backward pass\n","  l.backward() # dl/dw\n","\n","  # update weights\n","  optimizer.step()\n","\n","  # zero gradients\n","  optimizer.zero_grad()\n","\n","  if epoch % 10 == 0:\n","    print(f'epoch {epoch+1}, loss = {l:.8f}')\n","\n","print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["4 1\n","epoch 1, loss = 27.93873978\n","epoch 11, loss = 0.00007824\n","epoch 21, loss = 0.00006234\n","epoch 31, loss = 0.00004970\n","epoch 41, loss = 0.00003962\n","epoch 51, loss = 0.00003159\n","epoch 61, loss = 0.00002519\n","epoch 71, loss = 0.00002008\n","epoch 81, loss = 0.00001602\n","epoch 91, loss = 0.00001277\n","Prediction before training: f(5) = 10.005\n"],"name":"stdout"}]}]}