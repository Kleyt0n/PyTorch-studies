{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11_activation_functions.ipynb","provenance":[],"authorship_tag":"ABX9TyNttKTuJd3BwC2kBy71wg8M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rgA_mxp1UFw4"},"source":["# Activation Functions\n","\n","- With non-linear transformations the netwrks can learn better and perform more comples tasks;\n","- After each layers typically use an activation function;\n","\n","- Types of activation functions\n"," - Step function\n"," $$f(x)=1 if x\\geq \\theta; 0 \\text{ otherwise} $$\n"," - Sigmoid \n"," $$f(x) = \\frac{1}{1+e^{-x}}$$\n"," - TanH function\n"," $$f(x) = \\frac{2}{1+e^{-2x}}-1$$\n"," - ReLU function\n"," $$f(x) = max(0,x)$$\n"," - Leaky ReLU function\n"," $$f(x) = x \\text{ if x }\\geq 0; a.x \\text{ otherwise }$$\n"," - Softmax function\n"," $$S(y_{i})=\\frac{e^{y_{i}}}{\\sum e^{y_{i}}}$$\n"]},{"cell_type":"code","metadata":{"id":"FPRz7Wumw3Dp","executionInfo":{"status":"ok","timestamp":1627503486320,"user_tz":180,"elapsed":3724,"user":{"displayName":"Kleyton Costa","photoUrl":"","userId":"09554574939383241099"}}},"source":["#libraries\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"__5oxMQAxDYL"},"source":["# 1 create nn Modules\n","\n","class NeuralNet(nn.Module):\n","  def __init__(self, input_size, hidden_size):\n","    super(NeuralNet, self).__init__()\n","    self.linear1 = nn.Linear(input_size, hidden_size)\n","    self.relu = nn.ReLU()\n","    self.linear2 = nn.Linear(hidden_size, 1)\n","    self.sigmoid = nn.sigmoid()\n","\n","  def forward(self, x):\n","    out = self.linear1(x)\n","    out = self.relu(out)\n","    out = self.linear2(out)\n","    out = self.sigmoid(out)\n","    return out\n","\n","# 2 use activation function directly in forward pass\n","\n","class NeuralNet(nn.Module):\n","  def __init__(self, input_size, hidden_size):\n","    super(NeuralNet, self).__init__()\n","    self.linear1 = nn.Linear(input_size, hidden_size)\n","    self.linear2 = nn.Linear(hidden_size, 1)\n","F.\n","  def forward(self, x):\n","    out = torch.relu(self.linear1(x)) # torch.softmax, torch.tanh\n","    out = torch.sigmoid(self.linear2(out))\n","    return out"],"execution_count":null,"outputs":[]}]}