{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"4_gradient_descent.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"mNmYQ34UyjnI"},"source":["# Gradient Descendent \n","\n","## Gradients numpy"]},{"cell_type":"code","metadata":{"id":"CDuTI1RD0bVb","executionInfo":{"status":"ok","timestamp":1626917166481,"user_tz":180,"elapsed":240,"user":{"displayName":"Kleyton da Costa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4FvI3FVxRTumtqHl6_ESWSialxZIC43ACeXfI3A=s64","userId":"04061528204186357662"}}},"source":["import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgJOXJK5yjnO","executionInfo":{"status":"ok","timestamp":1626917665669,"user_tz":180,"elapsed":255,"user":{"displayName":"Kleyton da Costa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4FvI3FVxRTumtqHl6_ESWSialxZIC43ACeXfI3A=s64","userId":"04061528204186357662"}}},"source":["# f = w * x\n","\n","# f = 2 * x\n","\n","x = np.array([1,2,3,4], dtype=np.float32)\n","y = np.array([2,4,6,8], dtype=np.float32)\n","\n","w = 0.0"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDe1qxHEzZCF","executionInfo":{"status":"ok","timestamp":1626917668145,"user_tz":180,"elapsed":556,"user":{"displayName":"Kleyton da Costa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4FvI3FVxRTumtqHl6_ESWSialxZIC43ACeXfI3A=s64","userId":"04061528204186357662"}},"outputId":"78e284a2-caaa-477d-f6be-9bd6b20f6ff9"},"source":["# model prediction\n","\n","def forward(x):\n","  return w * x\n","\n","# loss = MSE\n","def loss(y, y_pred):\n","  return((y_pred-y)**2).mean()\n","\n","# gradient\n","# MSE = 1/N * (w*x-y)**2\n","# dJ/dw = 1/N 2x ()\n","\n","def gradient(x,y,y_pred):\n","  return np.dot(2*x, y_pred-y).mean()\n","\n","print(f'Prediction before training: f(5) = {forward(5):.3f}')\n","\n","# training\n","learning_rate = 0.01\n","n_iters = 100\n","\n","for epoch in range(n_iters):\n","  # prediction = forward pass\n","  y_pred = forward(x)\n","\n","  # loss\n","  l = loss(y, y_pred)\n","\n","  # gradients\n","  dw = gradient(x,y,y_pred)\n","\n","  # update weights\n","  w -= learning_rate * dw\n","\n","  if epoch % 3 == 0:\n","    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n","\n","print(f'Prediction before training: f(5) = {forward(5):.3f}')\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Prediction before training: f(5) = 0.000\n","epoch 1: w = 1.200, loss = 30.00000000\n","epoch 4: w = 1.949, loss = 0.12288000\n","epoch 7: w = 1.997, loss = 0.00050331\n","epoch 10: w = 2.000, loss = 0.00000206\n","epoch 13: w = 2.000, loss = 0.00000001\n","epoch 16: w = 2.000, loss = 0.00000000\n","epoch 19: w = 2.000, loss = 0.00000000\n","epoch 22: w = 2.000, loss = 0.00000000\n","epoch 25: w = 2.000, loss = 0.00000000\n","epoch 28: w = 2.000, loss = 0.00000000\n","epoch 31: w = 2.000, loss = 0.00000000\n","epoch 34: w = 2.000, loss = 0.00000000\n","epoch 37: w = 2.000, loss = 0.00000000\n","epoch 40: w = 2.000, loss = 0.00000000\n","epoch 43: w = 2.000, loss = 0.00000000\n","epoch 46: w = 2.000, loss = 0.00000000\n","epoch 49: w = 2.000, loss = 0.00000000\n","epoch 52: w = 2.000, loss = 0.00000000\n","epoch 55: w = 2.000, loss = 0.00000000\n","epoch 58: w = 2.000, loss = 0.00000000\n","epoch 61: w = 2.000, loss = 0.00000000\n","epoch 64: w = 2.000, loss = 0.00000000\n","epoch 67: w = 2.000, loss = 0.00000000\n","epoch 70: w = 2.000, loss = 0.00000000\n","epoch 73: w = 2.000, loss = 0.00000000\n","epoch 76: w = 2.000, loss = 0.00000000\n","epoch 79: w = 2.000, loss = 0.00000000\n","epoch 82: w = 2.000, loss = 0.00000000\n","epoch 85: w = 2.000, loss = 0.00000000\n","epoch 88: w = 2.000, loss = 0.00000000\n","epoch 91: w = 2.000, loss = 0.00000000\n","epoch 94: w = 2.000, loss = 0.00000000\n","epoch 97: w = 2.000, loss = 0.00000000\n","epoch 100: w = 2.000, loss = 0.00000000\n","Prediction before training: f(5) = 10.000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1ZnK-Fc32fqA"},"source":["## Gradients torch"]},{"cell_type":"code","metadata":{"id":"MdbK4H_H2kc1","executionInfo":{"status":"ok","timestamp":1626917730480,"user_tz":180,"elapsed":4855,"user":{"displayName":"Kleyton da Costa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4FvI3FVxRTumtqHl6_ESWSialxZIC43ACeXfI3A=s64","userId":"04061528204186357662"}}},"source":["import torch"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JOMPZbSF2tgK","executionInfo":{"status":"ok","timestamp":1626919200711,"user_tz":180,"elapsed":264,"user":{"displayName":"Kleyton da Costa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi4FvI3FVxRTumtqHl6_ESWSialxZIC43ACeXfI3A=s64","userId":"04061528204186357662"}},"outputId":"ac2a2215-5f48-4a81-8e5e-cb22bf98eded"},"source":["# f = w * x\n","\n","# f = 2 * x\n","\n","x = torch.tensor([1,2,3,4], dtype=torch.float32)\n","y = torch.tensor([2,4,6,8], dtype=torch.float32)\n","\n","w = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)\n","\n","# model prediction\n","\n","def forward(x):\n","  return w * x\n","\n","# loss = MSE\n","def loss(y, y_pred):\n","  return((y_pred-y)**2).mean()\n","\n","# gradient\n","# MSE = 1/N * (w*x-y)**2\n","# dJ/dw = 1/N 2x ()\n","\n","def gradient(x,y,y_pred):\n","  return np.dot(2*x, y_pred-y).mean()\n","\n","print(f'Prediction before training: f(5) = {forward(5):.3f}')\n","\n","# training\n","learning_rate = 0.01\n","n_iters = 10\n","\n","for epoch in range(n_iters):\n","  # prediction = forward pass\n","  y_pred = forward(x)\n","\n","  # loss\n","  l = loss(y, y_pred)\n","\n","  # gradients = backward pass\n","  l.backward() # dl/dw\n","\n","  # update weights\n","  with torch.no_grad():\n","    w -= learning_rate * w.grad\n","\n","  # zero gradients\n","  w.grad.zero_()\n","\n","  if epoch % 1 == 0:\n","    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n","\n","print(f'Prediction before training: f(5) = {forward(5):.3f}')\n"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Prediction before training: f(5) = 0.000\n","epoch 1: w = 0.300, loss = 30.00000000\n","epoch 2: w = 0.555, loss = 21.67499924\n","epoch 3: w = 0.772, loss = 15.66018772\n","epoch 4: w = 0.956, loss = 11.31448650\n","epoch 5: w = 1.113, loss = 8.17471695\n","epoch 6: w = 1.246, loss = 5.90623236\n","epoch 7: w = 1.359, loss = 4.26725292\n","epoch 8: w = 1.455, loss = 3.08308983\n","epoch 9: w = 1.537, loss = 2.22753215\n","epoch 10: w = 1.606, loss = 1.60939169\n","Prediction before training: f(5) = 8.031\n"],"name":"stdout"}]}]}