{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"5_training_pipeline.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"mNmYQ34UyjnI"},"source":["# Gradient Descendent "]},{"cell_type":"markdown","metadata":{"id":"1ZnK-Fc32fqA"},"source":["## Training Pipeline"]},{"cell_type":"code","metadata":{"id":"MdbK4H_H2kc1","executionInfo":{"status":"ok","timestamp":1626971073033,"user_tz":180,"elapsed":476,"user":{"displayName":"Kleyton Costa","photoUrl":"","userId":"09554574939383241099"}}},"source":["# 1) design model (input, output size, forward pass)\n","# 2) Construct loss and optimizer\n","# 3) Training loop\n","#   - forward pass: compute prediction\n","#   - backward pass: gradients\n","#   - update weights\n","\n","import torch\n","import torch.nn as nn"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JOMPZbSF2tgK","executionInfo":{"status":"ok","timestamp":1626971489659,"user_tz":180,"elapsed":410,"user":{"displayName":"Kleyton Costa","photoUrl":"","userId":"09554574939383241099"}},"outputId":"93c442e7-0dc1-460b-afd7-4d63e9532183"},"source":["# f = w * x\n","\n","# f = 2 * x\n","\n","x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n","y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n","\n","n_samples, n_features = x.shape\n","print(n_samples, n_features)\n","\n","x_test = torch.tensor([5], dtype=torch.float32)\n","input_size = n_features\n","output_size = n_features\n","model = nn.Linear(input_size, output_size)\n","\n","class LinearRegression(nn.Module):\n","  def __init__(self, input_dim, output_dim):\n","    super(LinearRegression, self).__init__()\n","    # define layers\n","    self.lin = nn.Linear(input_dim, output_dim)\n","  \n","  def forward(self, x):\n","    return self.lin(x)\n","\n","model = LinearRegression(input_size, output_size)\n","\n","print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n","\n","# training\n","learning_rate = 0.01\n","n_iters = 100\n","\n","loss =  nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) # stochastic gradient descent\n","\n","for epoch in range(n_iters):\n","  # prediction = forward pass\n","  y_pred = model(x)\n","\n","  # loss\n","  l = loss(y, y_pred)\n","\n","  # gradients = backward pass\n","  l.backward() # dl/dw\n","\n","  # update weights\n","  optimizer.step()\n","\n","  # zero gradients\n","  optimizer.zero_grad()\n","\n","  if epoch % 10 == 0:\n","    [w,b] = model.parameters()\n","    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n","\n","print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["4 1\n","Prediction before training: f(5) = -4.877\n","epoch 1: w = -0.444, loss = 68.74407959\n","epoch 11: w = 1.459, loss = 1.82273746\n","epoch 21: w = 1.770, loss = 0.08875199\n","epoch 31: w = 1.824, loss = 0.04146870\n","epoch 41: w = 1.837, loss = 0.03796528\n","epoch 51: w = 1.843, loss = 0.03572734\n","epoch 61: w = 1.848, loss = 0.03364708\n","epoch 71: w = 1.852, loss = 0.03168865\n","epoch 81: w = 1.857, loss = 0.02984422\n","epoch 91: w = 1.861, loss = 0.02810709\n","Prediction before training: f(5) = 9.721\n"],"name":"stdout"}]}]}